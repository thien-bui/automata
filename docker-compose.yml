services:
  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - automata

  api:
    build:
      context: .
      dockerfile: apps/api/Dockerfile
    env_file:
      - apps/api/.env
    environment:
      - NODE_ENV=production
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "4000:4000"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:4000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - automata

  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    env_file:
      - apps/web/.env
    depends_on:
      api:
        condition: service_started
    ports:
      - "5173:80"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost/healthz || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - automata

  ollama:
    build:
      context: .
      dockerfile: deploy/ollama/Dockerfile
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ./deploy/ollama:/root/.ollama
      - ./deploy/ollama-entrypoint.sh:/entrypoint.sh:ro
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_DEFAULT_MODEL=llama3.2:latest
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - ROCR_VISIBLE_DEVICES=0
    entrypoint:
      - /entrypoint.sh
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      # - video
      # - render
      - 991
      - 41
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSL http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - automata

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    profiles:
      - llm
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "3000:8080"
    volumes:
      - ./deploy/open-webui:/app/backend/data
    restart: unless-stopped
    networks:
      - automata

  gateway:
    image: nginx:1.27-alpine
    profiles:
      - gateway
    volumes:
      - ./deploy/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      api:
        condition: service_started
      web:
        condition: service_started
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8080/healthz || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - automata

networks:
  automata:
    external: true
    name: automata
